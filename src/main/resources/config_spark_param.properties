#spark.master = spark://10.0.30.101:7077
#spark.master = local[*]
#spark.master = yarn
spark.app.name = app
#spark.submit.deployMode = client
spark.serializer = org.apache.spark.serializer.KryoSerializer
#es.index.auto.create = true
#es.nodes = 10.0.30.101,10.0.30.102,10.0.30.105,10.0.30.107
#es.port = 9200
#spark.cores.max = 200
#spark.executor.cores = 3
#spark.executor.memory = 1g
spark.sql.shuffle.partitions = 3000
#spark.executor.instances = 4
#spark.eventLog.enabled=true
#spark.eventLog.compress=true
#spark.eventLog.dir=file:///tmp/spark-events
#spark.history.fs.logDirectory=file:///tmp/spark-events
#spark.yarn.historyServer.address=10.0.30.101:18080
#spark.yarn.queue = scheduler
#/opt/apps/spark/bin/spark-submit --class cn.ac.iie.Main --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 2g --executor-cores 5 --num-executors 50 --queue scheduler  ~/Hours-0.1a.jar

#/opt/apps/bigdata/spark/bin/spark-submit --class cn.ac.iie.Main --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 2g --executor-cores 5 --num-executors 50 --queue scheduler  ~/workspace/ljy/dns.jar
